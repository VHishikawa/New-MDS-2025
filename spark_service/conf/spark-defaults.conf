# spark_service/conf/spark-defaults.conf

# Configurações do Spark para S3A e MinIO
spark.hadoop.fs.s3a.endpoint                      http://minio:9000
spark.hadoop.fs.s3a.access.key                    datalake
spark.hadoop.fs.s3a.secret.key                    datalake
spark.hadoop.fs.s3a.path.style.access             true
spark.hadoop.fs.s3a.impl                          org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.ssl.enabled        false # MUITO IMPORTANTE para MinIO local (sem HTTPS)

# Configurações de timeout e performance para S3A/MinIO
spark.hadoop.fs.s3a.connection.establish.timeout  5000
spark.hadoop.fs.s3a.connection.timeout            180000
spark.hadoop.fs.s3a.readahead.range               2M
spark.hadoop.fs.s3a.multipart.size                10M
spark.hadoop.fs.s3a.fast.upload                   true
spark.hadoop.fs.s3a.fast.upload.buffer            bytebuffer
spark.hadoop.fs.s3a.fast.upload.buffer            bytebuffer
spark.hadoop.fs.s3a.impl.disable.cache            true
spark.hadoop.fs.s3a.block.size                    128M

# Configuração do Master para que os workers e clients saibam onde conectar
spark.master                     spark://spark-master:7077

# Habilita o Spark History Server e define o diretório de logs de eventos
spark.eventLog.enabled           true
spark.eventLog.dir               file:///opt/spark-events
spark.history.fs.logDirectory    file:///opt/spark-events

# Define o diretório para o Spark SQL warehouse (útil para Delta Lake e tabelas)
spark.sql.warehouse.dir          file:///opt/spark-warehouse

# Configurações de memória padrão para Driver e Executor
spark.driver.memory              2g
spark.executor.memory            4g
spark.executor.cores             2
spark.cores.max                  4

# Não limita o tamanho do resultado do driver (pode ser perigoso para grandes resultados)
spark.driver.maxResultSize       0

# spark.jars.packages para Kafka e Avro (certifique-se de que as versões são compatíveis com seu Spark)
# O Apache Spark 3.5.6 usa Scala 2.12.
spark.jars.packages              org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.6,\
                                 org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.6,\
                                 org.apache.spark:spark-avro_2.12:3.5.6

# Desabilita logs excessivos do Hadoop/Spark para melhor visualização (opcional)
spark.driver.extraJavaOptions    -Dlog4j.configurationFile=file:///opt/spark/conf/log4j2.properties
spark.executor.extraJavaOptions  -Dlog4j.configurationFile=file:///opt/spark/conf/log4j2.properties

# As linhas de 'extraClassPath' para JARs do Hadoop/AWS foram removidas,
# pois a imagem Bitnami Spark já deve incluí-las.
# Se encontrar erros de 'ClassNotFoundException' para S3A/AWS SDK após essa alteração,
# poderemos reavaliar e adicionar as JARs necessárias.